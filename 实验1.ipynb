{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b64843-d183-423b-ab0d-722da60428a4",
   "metadata": {},
   "source": [
    "# PyTorch基本操作实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aab33e-0f82-46ca-a07e-a4189bbdb1dc",
   "metadata": {},
   "source": [
    "## 一、Pytorch基本操作考察"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98de66-fdc1-466f-b825-39add57e6a1b",
   "metadata": {},
   "source": [
    "### 1、使用 𝐓𝐞𝐧𝐬𝐨𝐫 初始化一个 𝟏×𝟑 的矩阵 𝑴 和一个 𝟐×𝟏 的矩阵 𝑵，对两矩阵进行减法操作（要求实现三种不同的形式），给出结果并分析三种方式的不同（如果出现报错，分析报错的原因），同时需要指出在计算过程中发生了什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2de277fd-3bfe-4da7-815c-00755c4d4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "M = torch.tensor([1,2,3])\n",
    "N = torch.tensor([[1],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5454cefe-984a-4f3b-9f87-5307f85b8aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方式一结果: \n",
      " tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "# 方式一：直接相减\n",
    "res1 = M - N\n",
    "print(\"方式一结果:\",'\\n',res1)\n",
    "# PyTorch启用广播机制，它自动调整N的形状以匹配M的形状。在这里，N被复制并扩展成2x3的矩阵\n",
    "# M被扩展成2x3矩阵然后进行相减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "faf788ff-9389-4a15-9b9e-6f6ee5fa29fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方式二结果: \n",
      " tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "# 方式二：torch.sub进行\n",
    "res2 = M.sub(N)\n",
    "print(\"方式二结果:\",'\\n',res2)\n",
    "# 方式二同样也利用了广播机制，与直接相减方式相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54935dcf-4c7b-4d30-8931-cbebfb9a906c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [3] doesn't match the broadcast shape [2, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m M \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m      4\u001b[0m N \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m----> 5\u001b[0m res3 \u001b[38;5;241m=\u001b[39m \u001b[43mM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m方式三结果:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,res3)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 由于广播机制会扩大张量M的在一维和二维上的长度，与inplace原地操作不兼容，所以会报错\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [3] doesn't match the broadcast shape [2, 3]"
     ]
    }
   ],
   "source": [
    "# 方式三：inplace函数\n",
    "import torch\n",
    "M = torch.tensor([1,2,3])\n",
    "N = torch.tensor([[1],[1]])\n",
    "res3 = M.sub_(N)\n",
    "print(\"方式三结果:\",'\\n',res3)\n",
    "# 由于广播机制会扩大张量M的在一维和二维上的长度，与inplace原地操作不兼容，所以会报错"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd2bc9-74a3-45b5-bcb6-777d4676bbf6",
   "metadata": {},
   "source": [
    "### 2、① 利用 𝐓𝐞𝐧𝐬𝐨𝐫 创建两个大小分别 𝟑×𝟐 和 𝟒×𝟐 的随机数矩阵 𝑷 和 𝑸 ，要求服从均值为0，标准差0.01为的正态分布；② 对第二步得到的矩阵 𝑸 进行形状变换得到 𝑸 的转置 𝑸^𝑻 ；③ 对上述得到的矩阵 𝑷 和矩阵 𝑸^𝑻 求矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ad758a5-b2e8-48ec-a1dd-d1a5f7318e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0074, -0.0048],\n",
      "        [-0.0020, -0.0115],\n",
      "        [ 0.0093, -0.0224]]) \n",
      " tensor([[ 7.8963e-03, -2.2350e-03],\n",
      "        [ 2.6952e-03,  2.3349e-02],\n",
      "        [-9.6502e-05,  1.4392e-02],\n",
      "        [-6.8178e-04, -7.6675e-04]])\n"
     ]
    }
   ],
   "source": [
    "# 问题1解答：\n",
    "import torch\n",
    "P = torch.normal(0,0.01,(3,2))\n",
    "Q = torch.normal(0,0.01,(4,2))\n",
    "print(P,'\\n',Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b52d96b3-ec0b-42f5-b7f9-33a634fefcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.8963e-03,  2.6952e-03, -9.6502e-05, -6.8178e-04],\n",
      "        [-2.2350e-03,  2.3349e-02,  1.4392e-02, -7.6675e-04]])\n"
     ]
    }
   ],
   "source": [
    "# 问题二解答：获得Q的转置\n",
    "Q = Q.T\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92219a85-5657-47bc-b782-dd9bc3ba1e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.9028e-05, -9.2042e-05, -6.9715e-05, -1.3586e-06],\n",
       "        [ 9.6982e-06, -2.7486e-04, -1.6584e-04,  1.0235e-05],\n",
       "        [ 1.2397e-04, -4.9866e-04, -3.2380e-04,  1.0828e-05]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 问题三解答：矩阵相乘\n",
    "torch.mm(P,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b2166-8fc4-45c5-a8cc-9eaf5823b3a0",
   "metadata": {},
   "source": [
    "### 3、给定公式 𝑦_3=𝑦_1+𝑦_2=𝑥^2+𝑥^3，且 𝑥=1。利用学习所得到的Tensor的相关知识，求𝑦_3对𝑥的梯度，即(𝑑𝑦_3)/𝑑𝑥。要求在计算过程中，在计算〖 𝑥〗^3 时中断梯度的追踪，观察结果并进行原因分析 提示, 可使用 with torch.no_grad()， 举例: with torch.no_grad(): y = x * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e846fe9-ae5e-4061-8573-794158183089",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1,dtype=torch.float64,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a9b7b28-1979-439e-8b21-1113cf8629b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y1=x*x\n",
    "y2=x*x*x\n",
    "y3=y1+y2\n",
    "y3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c273b25-45ec-4b6e-8e1a-3c49c0fbc147",
   "metadata": {},
   "source": [
    "## 二、动手实现 logistic 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4bc26-ead0-4e45-a5d3-5c0868ac86c1",
   "metadata": {},
   "source": [
    "### 1、要求动手从0实现 logistic 回归（只借助Tensor和Numpy相关的库）在人工构造的数据集上进行训练和测试，并从loss以及训练集上的准确率等多个角度对结果进行分析（可借助nn.BCELoss或nn.BCEWithLogitsLoss作为损失函数，从零实现二元交叉熵为选作）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8236fe5-c915-4096-a9a0-da50e6e8caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37df8371-8a4f-415a-9aa8-cc082e093942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一些数据集\n",
    "def generate_data(w,b,num_examples):\n",
    "    X = torch.normal(0,1,(num_examples,len(w)))\n",
    "    z = torch.matmul(X,w) + b\n",
    "    z += torch.normal(0,0.01,z.shape)\n",
    "    y = 1.0/(1.0+(-z).exp())\n",
    "    y = (y > 0.5).int()\n",
    "    return X,y.reshape((-1,1))\n",
    "true_w = torch.tensor([[2],[-3.4]])\n",
    "true_b = 4.2\n",
    "features,labels = generate_data(true_w,true_b,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0e9b010-d812-48fe-8775-5f051972e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size,features,labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0,num_examples,batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i+batch_size,num_examples)])\n",
    "        yield features[batch_indices],labels[batch_indices]\n",
    "        \n",
    "batch_size = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5cdb09fc-247c-4a3a-a3e2-c17f8f8ac1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "def logistic(X,w,b):\n",
    "    return 1.0/(1.0+(torch.matmul(X,w) + b).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "489bb14e-7e4b-4647-ad18-d4bf7b49c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "from torch import nn\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c503ef84-cf71-4d78-a73f-3d4e235e5111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化模型\n",
    "def sgd(params,lr,batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr*param.grad/batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a30e7a12-1301-4816-aa3c-766835a24f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.684979\n",
      "epoch 2, loss 0.679481\n",
      "epoch 3, loss 0.674068\n"
     ]
    }
   ],
   "source": [
    "w = torch.normal(0,0.01,size = (2,1),requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "lr = 0.003\n",
    "num_epochs=3\n",
    "net = logistic\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        l = loss(net(X,w,b),y.float())\n",
    "        l.sum().backward()\n",
    "        sgd([w,b],lr,batch_size)\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels.float())\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b1e7569-4261-4b73-aa4d-cf94c86bfcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: tensor([[ 2.0258],\n",
      "        [-3.4211]], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([4.2326], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a6c63-d03d-4581-a826-3a88ee514ce0",
   "metadata": {},
   "source": [
    "## 2、利用 torch.nn 实现 logistic 回归在人工构造的数据集上进行训练和测试，并对结果进行分析，并从loss以及训练集上的准确率等多个角度对结果进行分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1194f644-acf9-41f7-98bd-43b4017602ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f9b74a0-2bdf-44af-930f-cc26c1fcfe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一些数据集\n",
    "def generate_data(w,b,num_examples):\n",
    "    X = torch.normal(0,1,(num_examples,len(w)))\n",
    "    z = torch.matmul(X,w) + b\n",
    "    z += torch.normal(0,0.01,z.shape)\n",
    "    y = 1.0/(1.0+(-z).exp())\n",
    "    y = (y > 0.5).int()\n",
    "    return X,y.reshape((-1,1))\n",
    "true_w = torch.tensor([[2],[-3.4]])\n",
    "true_b = 4.2\n",
    "features,labels = generate_data(true_w,true_b,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb4432-64cb-44bb-b709-ba771a28cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_array,batch_size,is_train=True):\n",
    "    dataset = TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset,batch_size,shuffle=is_train)\n",
    "batch_size = 10\n",
    "data_iter = load_array((features,labels),batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
